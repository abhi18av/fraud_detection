---
title: "scratch"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath("/Users/eklavya/projects/education/formalEducation/DataScience/DataScienceAssignments/FinalProject/fraud_detection/"))  
```



```{python}

import os
os.chdir("/Users/eklavya/projects/education/formalEducation/DataScience/DataScienceAssignments/FinalProject/fraud_detection/")


```


# Credit Card Fraud Detection

 
The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.
 
It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
 
Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.
 



```{python}
 

# import the libraries

import pandas as pd 
import numpy as np 
import seaborn as sns 
import matplotlib.pyplot as plt # 
import matplotlib.gridspec as gridspec 

from imblearn.pipeline import make_pipeline as make_pipeline_imb 
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import make_pipeline
from imblearn.metrics import classification_report_imbalanced

from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from collections import Counter

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier



from sklearn.metrics import precision_score, recall_score, fbeta_score, confusion_matrix, precision_recall_curve, accuracy_score



def print_ln():
    print('-' * 80, '\n')


# the function that we will use to better evaluate the model
def print_results(headline, true_value, pred):
    print(headline)
    print("accuracy: {}".format(accuracy_score(true_value, pred)))
    print("precision: {}".format(precision_score(true_value, pred)))
    print("recall: {}".format(recall_score(true_value, pred)))
    print("f2: {}".format(fbeta_score(true_value, pred, beta=2)))



# loading the data
df_credit = pd.read_csv('./data/raw/creditcard.csv')


# looking the how data looks
df_credit.head()


# looking the type and searching for null values
df_credit.info()


# Firstly, I will explore through 3 different columns:
# - Time
# - Amount
# - Class


# explore the "normal" columns
df_credit[["Time","Amount","Class"]].describe()


```


```{python}
#Lets start looking the difference by Normal and Fraud transactions
print("Distribuition of Normal(0) and Frauds(1): ")
print(df_credit["Class"].value_counts())

# We have a clearly imbalanced data.
plt.figure(figsize=(7,5))
sns.countplot(df_credit['Class'])
plt.title("Class Count", fontsize=18)
plt.xlabel("Is fraud?", fontsize=15)
plt.ylabel("Count", fontsize=15)
plt.show()

```



```{python}

# Firstly, do some explore through the Time and Amount. 
# Secondly, explore the V Features, that are already transformed via PCA

# Feature Engineering

# As our Time feature are in seconds we will transform it ot minutes and hours to get a better understand of the patterns


timedelta = pd.to_timedelta(df_credit['Time'], unit='s')
df_credit['Time_min'] = (timedelta.dt.components.minutes).astype(int)
df_credit['Time_hour'] = (timedelta.dt.components.hours).astype(int)


# exploring the distribuition by Class types through hours and minutes
plt.figure(figsize=(12,5))
sns.distplot(df_credit[df_credit['Class'] == 0]["Time_hour"],  color='g')
sns.distplot(df_credit[df_credit['Class'] == 1]["Time_hour"],  color='r')
plt.title('Fraud x Normal Transactions by Hours', fontsize=17)
plt.xlim([-1,25])
plt.show()



# exploring the distribuition by Class types throught hours and minutes
plt.figure(figsize=(12,5))
sns.distplot(df_credit[df_credit['Class'] == 0]["Time_min"],  color='g')
sns.distplot(df_credit[df_credit['Class'] == 1]["Time_min"],  color='r')
plt.title('Fraud x Normal Transactions by minutes', fontsize=17)
plt.xlim([-1,61])
plt.show()

```


```{python}

# looking the statistics of our Amount class frauds and normal transactions

df_fraud = df_credit[df_credit['Class'] == 1]
df_normal = df_credit[df_credit['Class'] == 0]

print("Fraud transaction statistics")
print(df_fraud["Amount"].describe())
print("\nNormal transaction statistics")
print(df_normal["Amount"].describe())

```



```{python}
# Using this informations I will filter the values to look for Amount by Class <br>
# I will filter the "normal" amounts by 3.000


#Feature engineering to a better visualization of the values
df_credit['Amount_log'] = np.log(df_credit.Amount + 0.01)



plt.figure(figsize=(14,6))
#I will explore the Amount by Class and see the distribuition of Amount transactions
plt.subplot(121)
ax = sns.boxplot(x ="Class",y="Amount",
                 data=df_credit)
ax.set_title("Class x Amount", fontsize=20)
ax.set_xlabel("Is Fraud?", fontsize=16)
ax.set_ylabel("Amount(US)", fontsize = 16)

plt.subplot(122)
ax1 = sns.boxplot(x ="Class",y="Amount_log", data=df_credit)
ax1.set_title("Class x Amount", fontsize=20)
ax1.set_xlabel("Is Fraud?", fontsize=16)
ax1.set_ylabel("Amount(Log)", fontsize = 16)

plt.subplots_adjust(hspace = 0.6, top = 0.8)

plt.show()

```


```{python}
# We can see a slightly difference in log amount of our two Classes. <br>
# The IQR of fraudulent transactions are higher than normal transactions, but normal transactions have highest values

# Looking a scatter plot of the Time_min distribuition by Amount

#Looking the Amount and time distribuition of FRAUD transactions
ax = sns.lmplot(y="Amount", x="Time_min", fit_reg=False,aspect=1.8,
                data=df_credit, hue='Class')
plt.title("Amounts by Minutes of Frauds and Normal Transactions",fontsize=16)
plt.show()

```



```{python}
# Looking a scatter plot of the Time_hour distribuition by Amount


ax = sns.lmplot(y="Amount", x="Time_hour", fit_reg=False,aspect=1.8,
                data=df_credit, hue='Class')
plt.title("Amounts by Hour of Frauds and Normal Transactions", fontsize=16)

plt.show()

```

```{python}

# I will use boxplot to search differents distribuitions: 
# We are searching for features that diverges from normal distribution


#Looking the V's features
columns = df_credit.iloc[:,1:29].columns

frauds = df_credit.Class == 1
normals = df_credit.Class == 0

grid = gridspec.GridSpec(14, 2)
plt.figure(figsize=(15,20*4))

for n, col in enumerate(df_credit[columns]):
    ax = plt.subplot(grid[n])
    sns.distplot(df_credit[col][frauds], bins = 50, color='g') #Will receive the "semi-salmon" violin
    sns.distplot(df_credit[col][normals], bins = 50, color='r') #Will receive the "ocean" color
    ax.set_ylabel('Density')
    ax.set_title(str(col))
    ax.set_xlabel('')
plt.show()

# We can see a interesting different distribuition in some of our features like V4, V9, V16, V17 and a lot more.  <br>
# Now let's take a look on time distribuition

# Diference in time


```


```{python}


# ## Feature selections

# I will select the variables where fraud class have a interesting behavior and might can help us predict

df_credit = df_credit[["Time_hour","Time_min","V2","V3","V4","V9","V10","V11","V12","V14","V16","V17","V18","V19","V27","Amount","Class"]]


# Feature Engineering


df_credit.Amount = np.log(df_credit.Amount + 0.001)

#Looking the final df
df_credit.head()
```



```{python}

colormap = plt.cm.Greens

plt.figure(figsize=(14,12))

sns.heatmap(df_credit.corr(),linewidths=0.1,vmax=1.0, 
            square=True, cmap = colormap, linecolor='white', annot=True)
plt.show()

```

```{python}

## Preprocessing


X = df_credit.drop(["Class"], axis=1).values #Setting the X to do the split
y = df_credit["Class"].values # transforming the values in array


# splitting data into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=0.20)

```

```{python}


model_rf_1= RandomForestClassifier(n_estimators= 100, random_state = 100)

# Train the model using the training sets y_pred=clf.predict(X_test)
model_rf_1.fit(X_train, y_train)

y_pred= model_rf_1.predict(X_test)


print("Confusion Matrix: ")
print(confusion_matrix(y_test, y_pred))

print('\nPipeline Score {}'.format(model_rf_1.score(X_test, y_test)))

print_results("\nmodel_rf_1 classification", y_test, y_pred)

# Compute predicted probabilities: y_pred_prob
y_pred_prob = model_rf_1.predict_proba(X_test)[:,1]

# Generate precision recall curve values: precision, recall, thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot(precision, recall)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')
plt.show()


```


```{python}

# Creating the model 
model_lr_1 = LogisticRegression(C=10, penalty='l2', random_state=2)

#Fiting the model
model_lr_1.fit(X_train, y_train)
           
# Printing the Training Score
print("Cross Validation of X and y Train: ")
print(cross_val_score(model_lr_1,X_train, y_train, cv=5, scoring='recall'))


print("Confusion Matrix: ")
print(confusion_matrix(y_test, y_pred))

print('\nPipeline Score {}'.format(model_lr_1.score(X_test, y_test)))

print_results("\nmodel_lr_1 classification", y_test, y_pred)

# Compute predicted probabilities: y_pred_prob
y_pred_prob = model_lr_1.predict_proba(X_test)[:,1]

# Generate precision recall curve values: precision, recall, thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot(precision, recall)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')
plt.show()

 

```





```{python}

# build model with SMOTE imblearn
model_rf_2 = RandomForestClassifier
smote_pipeline = make_pipeline_imb(SMOTE(random_state=4), model_rf_2(random_state=42))

smote_model = smote_pipeline.fit(X_train, y_train)
smote_prediction = smote_model.predict(X_test)

#Showing the diference before and after the transformation used
print("normal data distribution: {}".format(Counter(y)))
X_smote, y_smote = SMOTE().fit_sample(X, y)
print("SMOTE data distribution: {}".format(Counter(y_smote)))


# ## Evaluating the model SMOTE + Random Forest


print("Confusion Matrix: ")
print(confusion_matrix(y_test, smote_prediction))

print('\nSMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))

print_results("\nSMOTE + RandomForest classification", y_test, smote_prediction)

# Compute predicted probabilities: y_pred_prob
y_pred_prob = smote_pipeline.predict_proba(X_test)[:,1]

# Generate precision recall curve values: precision, recall, thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot(precision, recall)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')
plt.show()


```



```{python}

# build model with SMOTE imblearn
model_lr_2 = LogisticRegression
 
smote_pipeline = make_pipeline_imb(SMOTE(random_state=4), model_lr_2(random_state=42))

smote_model = smote_pipeline.fit(X_train, y_train)
smote_prediction = smote_model.predict(X_test)

#Showing the diference before and after the transformation used
print("normal data distribution: {}".format(Counter(y)))
X_smote, y_smote = SMOTE().fit_sample(X, y)
print("SMOTE data distribution: {}".format(Counter(y_smote)))

# Evaluating the model SMOTE 

print("Confusion Matrix: ")
print(confusion_matrix(y_test, smote_prediction))

print('\nSMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))

print_results("\nSMOTE + model_lr_2 classification", y_test, smote_prediction)

# Compute predicted probabilities: y_pred_prob
y_pred_prob = smote_pipeline.predict_proba(X_test)[:,1]

# Generate precision recall curve values: precision, recall, thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot(precision, recall)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')
plt.show()


```




```{python}

#params of the model
param_grid = {"max_depth": [3,5, None],
              "n_estimators":[3,5,10],
              "max_features": [5,6,7,8]}

# Creating the classifier
model_rf_3 = RandomForestClassifier(max_features=3, max_depth=2 ,n_estimators=10, random_state=3, criterion='entropy', n_jobs=1, verbose=1 )


grid_search = GridSearchCV(model_rf_3, param_grid=param_grid, cv=5, scoring='recall')
grid_search.fit(X_train, y_train)


print(grid_search.best_score_)
print(grid_search.best_params_)
```



```{python}

# l1 lasso l2 ridge
param_grid= {"C":np.logspace(-3,3,7), 
             "penalty":["l2"]}

model_lr_3= LogisticRegression(max_iter=200)

logreg_cv= GridSearchCV(model_lr_3, param_grid, cv=10)
logreg_cv.fit(X_train, y_train)

print("tuned hpyerparameters :(best parameters) ",logreg_cv.best_params_)
print("accuracy :",logreg_cv.best_score_)

grid_search = GridSearchCV(model_lr_3, param_grid=param_grid, cv=5, scoring='recall')
grid_search.fit(X_train, y_train)

print(grid_search.best_score_)
print(grid_search.best_params_)
```




```{python}

# ## Feature importance plot

features = ["Time_min", 'Time_hours',"V2","V3","V4","V9","V10","V11","V12","V14","V16","V17","V18","V19","V27","Amount"]

plt.figure(figsize = (9,5))

feat_import = pd.DataFrame({'Feature': features, 'Feature importance': model_rf_4.feature_importances_})
feat_import = feat_import.sort_values(by='Feature importance',ascending=False)

g = sns.barplot(x='Feature',y='Feature importance',data=feat_import)
g.set_xticklabels(g.get_xticklabels(),rotation=90)
g.set_title('Features importance - Random Forest',fontsize=20)
plt.show() 


# The top 4 feature are V17, V14, V12, V10 corresponds to 75% of total. 
 
# Also the f2 score that is the median of recall and precision are on a considerably value

# ROC CURVE - Random Forest

# Predicting proba
y_pred_prob = model_rf_4.predict_proba(X_test)[:,1]

# Generate precision recall curve values: precision, recall, thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot(precision, recall)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')
plt.show()


results = cross_val_score(model_rf_4,X_train, y_train, cv=10, scoring='recall')
results


```


```{python}
# Modelling Logistic Regression with Hyper Parameters

param_grid = {'C': [0.01, 0.1, 1, 10],
             'penalty':['l1', 'l2']}

model_lr_1 = LogisticRegression(random_state=2)

grid_search_lr = GridSearchCV(model_lr_1, param_grid=param_grid, scoring='recall', cv=5)

grid_search_lr.fit(X_train, y_train)

# The best recall obtained
print(grid_search_lr.best_score_)
#Best parameter on trainning set
print(grid_search_lr.best_params_)


# Setting the best parameters as parameters of our model

# Creating the model 
model_lr_2 = LogisticRegression(C=10, penalty='l2',random_state=2)

#Fiting the model
model_lr_2.fit(X_train, y_train)
           
# Printing the Training Score
print("Cross Validation of X and y Train: ")
print(cross_val_score(model_lr_2,X_train, y_train, cv=5, scoring='recall'))


# Predicting with the best params
y_pred = model_lr_2.predict(X_test)

print(confusion_matrix(y_test, y_pred))

print_results("model_lr_2 classification", y_test, y_pred)


# 70% of accuracy is not too bad, but we found a high vale on the Random Forest Model

# Precision Recall Curve of Logistic Regression


# Predicting proba
y_pred_prob = model_lr_2.predict_proba(X_test)[:,1]

# Generate precision recall curve values: precision, recall, thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot(precision, recall)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')
plt.show()

 

```


# CONCLUSION: 
The highest values of Normal transactions are 25691.16 while of Fraudulent transactions are just 2125.87. <br>
The average value of normal transactions are small(USD 88.29) than fraudulent transactions that is USD 122.21
 
We got the best score when we use the SMOTE (OverSampling)  + RandomForest, that performed a f2 score of 0.8669~ 
This is a considerably difference by the second best model that is 0.8252 that uses just RandomForests with some Hyper Parameters.
The worst model was model_lr_1 where I used GridSearchCV to get the Best params to fit and predict where the recall was ~0.6666 and f2 ~0.70.
 


# TODO only do k-fold analysis only for the final model test-train split 
# TODO https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833




